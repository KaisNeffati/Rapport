%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Implementation}
\label{chap:intro}

\initial{I}n this section we will describe our project work. 
We will start by presenting the working environment . Then we move to talk about each part of
our project , presenting screenshots and giving the examples of each Implementation.
%=======
\section{Work environnement}
\label{sec:sec01}
This part presents the Hardware and Software environment available for this project.
\subsection{Hardware environnement}
\label{sec:sec01}
In our project we developed our application using a computer with the following characteristics : 
\begin{itemize}
  \item Process Intel Core i5 4310M .
  \item Memory RAM: 16 GB .
  \item Space Discs: 500 GB.
\end{itemize}

\section{Software environnement}
\label{sec:sec01}
We've used for our project the following Software :
\begin{itemize}
  \item VMware Workstation : Our application is deployed under VMware Workstation virtualization
   platform as a set of decoupled service. Each service per container.
  \item Centos 7 : Our application operating system is Centos 7 carring Docker as demon. 
  \item Docker : is our main virtualization tool and our containers manager.
  \item Intellij idea : is our application Back End IDE , we used it for Scala and java (spring) developement.
  \item Visual Studio Code : is our application Front End IDE , we used it for angular 2 and webpack (TypeScript developement).
\end{itemize}
\section{Realized work}
\label{sec:sec01}
\subsection{Environnement preparation}
\label{sec:sec01}
Our application require some containers preparation. The following list contain all needed containers.
\begin{itemize}
  \item Kafka container : used for collecting data and stream them through a message queue. This container does not existe in dockerhub repository so we should Dockerize it(Dockerize
   kafka means create a container that contain kafka)
  \item Zookeeper container : used to save brokers address and messages offsets . this container exists in docker hub.
  \item Scala container : used to build the Hadoop container.This container does not existe in dockerhub repository so we should Dockerize it
  \item Hadoop container : used to build the spark container and provide HDFS persistance and YARN (ressource manager) for the spark jobs. This container does not existe with
   the Hadoop 2.7 and centos 7 version, so we should Dockerize it
  \item Spark container : used to to consume the data from kafka , process it then feed it to the serving layer (persisting results in cassandra).
  \item Cassandra container : used as a serving layer . this container exists in docker hub , we will be using the 2.2 cassandra version.
  \item Zeppelin container : used as a notebook to interpret Cassandra and spark then display the results in a dashboard.
  \item Spring boot containers : we need some containers that carry each one a microservice. This containers just need to have a JVM.
  \item Mongodb containers : each service is linked to a mongodb container. this containers exists in dockerhub.
  \item FrontEnd container : container for the angular dashboard.
\end{itemize}
\subsubsection{Dockernize Tools}
\label{sec:sec01}
In this section we present the process of dockernizing our containers .All installation setps are written 
in Dockerfiles and each container has an OS kernel. We will be using alpine and centos 7.
The next figure show all our docker images:\\
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/DockerImages}
	\mycaption[Docker images.]{Docker images.}
	\label{fig:FilialesEtClients}
\end{figure}

\paragraph{Dockernize Kafka}
\label{sec:sec01}
To dockernize this container, we will start by putting a base OS version wich is alpine linux . The next step is the Jdk installation. After fixing all paths we move to install
kafka.
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.2\textheight]{fig01/KafkaContainer}
	\mycaption[Kafka container.]{Kafka container.}
	\label{fig:FilialesEtClients}
\end{figure}
\paragraph{Dockernize Scala}
\label{sec:sec01}
To dockernize this container, we will start by putting a base OS version wich is centos 7 . 
The next step is the Jdk installation. After fixing all paths we move to install Scala.
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.2\textheight]{fig01/ScalaContainer}
	\mycaption[Scala container.]{Scala container.}
	\label{fig:FilialesEtClients}
\end{figure}
\paragraph{Dockernize Hadoop}
\label{sec:sec01}
To dockernize this container, we will start from the previous Scala container. 
The next step is the Hadoop installation which takes too much time due to the big configuration amount.
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.2\textheight]{fig01/HadoopContainer}
	\mycaption[Hadoop Container.]{Hadoop Container.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
After dockernizing Hadoop 2.7 we are able now to see some Doshboard.
Hadoop YARN ResourceManager HTTP UI :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.4\textheight]{fig01/HadoopYarnDashboard}
	\mycaption[Hadoop YARN ResourceManager HTTP UI.]{Hadoop YARN ResourceManager HTTP UI. }
	\label{fig:FilialesEtClients}
\end{figure}
\newline
Hadoop HDFS NameNode HTTP UI :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/HDFSNameNode}
	\mycaption[Hadoop HDFS NameNode HTTP UI.]{Hadoop HDFS NameNode HTTP UI.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
We can Check Files existance through the HDFS NameNode UI :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.5\textheight]{fig01/BrowserFilesHDFS}
	\mycaption[Browser files HDFS.]{Browser files HDFS.}
	\label{fig:FilialesEtClients}
\end{figure}
\paragraph{Dockernize Spark}
\label{sec:sec01}
This container is the main container. We built the last two ones for this step. This container is based on the haddop one . Spark uses HDFS to persist data
and uses YARN as a ResourceManager.This container require much more then the last one in terms of configuration. We chose to built spark on yarn to benefit the real sparks power. \\
So spark present in totaly 3 deployment mode :\\
Standalone deployment: With the standalone deployment one we can statically
allocate resources on all or a subset of machines in a Hadoop cluster and run
Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs
on his HDFS data. Its simplicity makes this the deployment of choice for many
Hadoop 1.x users.\\
Hadoop Yarn deployment: Hadoop users who have already deployed or are
planning to deploy Hadoop Yarn can simply run Spark on YARN without any pre-
installation or administrative access required. This allows users to easily
integrate Spark in their Hadoop stack and take advantage of the full power of
Spark, as well as of other components running on top of Spark.\\
Spark In MapReduce (SIMR): For the Hadoop users that are not running YARN
yet, another option, in addition to the standalone deployment, is to use SIMR to
launch Spark jobs inside MapReduce. With SIMR, users can start experimenting
with Spark and use its shell within a couple of minutes after downloading it! This
tremendously lowers the barrier of deployment, and lets virtually everyone play
with Spark.\\
\textit{
Notice:
 Spark is an in-Memory data processing framwork so when trying to configure it make sure you put the right RM consumption.
}
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.2\textheight]{fig01/SparkFinalContainer}
	\mycaption[Spark container.]{Spark container.}
	\label{fig:FilialesEtClients}
\end{figure}
\paragraph{Dockernize Zeppelin}
Zeppelin is a notebook that allow as to interpret spark or cassandra . Zeppelin container is built on a 
an alpine linux operating system.
\label{sec:sec01}
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.2\textheight]{fig01/ZeppelinContainer}
	\mycaption[Zeppelin container.]{Zeppelin container.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsection{Log Producer}
\label{sec:sec01}
The log producer is a Scala program that act as a producer for our kafka OSS. We ve used this log producer to perform
a real data stream flow.
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.6\textheight]{fig01/logProducer}
	\mycaption[Scala application log producer.]{Scala application log producer.}
	\label{fig:FilialesEtClients}
\end{figure}

\subsection{Spark Jobs}
\label{sec:sec01}
Applying the lambda architecture require running at least two jobs, in our case we ve created three spark jobs using Scala :
\begin{itemize}
\item BatchSaveTOHDFS: This job persists realtime data to HDFS. It consumes data flow from Kafka and persist it to HDFS continuously.
\item StreamingProcessing: This job is the realtime engine, it consumes data from Kafka and process it and gives results in realtime. 
\item BatchProcessing: This job is the batch engine, it processes the data already persisted in HDFS and gives results each 8 hours.
\end{itemize}
\subsubsection{BatchProcessing}
\label{sec:sec01}
The next figure show the BatchProcessing job in it's accepted status:

\begin{figure}[h!]
	\centering
	\includegraphics[height=0.6\textheight]{fig01/SparkJobAccepted}
	\mycaption[BatchProcessing spark job in accepted status.]{BatchProcessing spark job in accepted status.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
The next figure show the BatchProcessing job in it's running status:
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/SparkJobRunning}
	\mycaption[BatchProcessing spark job in running status.]{BatchProcessing spark job in running status.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsubsection{StreamingProcessing}
\label{sec:sec01}

The next figure show the StreamingProcessing job in it's accepted status:

\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/StreamingProcessingAccepted}
	\mycaption[StreamingProcessing spark job in accepted status.]{StreamingProcessing spark job in accepted status.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
The next figure show the BatchProcessing job in it's running status:
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/StreamingProcessingRunning}
	\mycaption[StreamingProcessing spark job in running status.]{StreamingProcessing spark job in running status.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsubsection{BatchSaveTOHDFS}
\label{sec:sec01}
The next figure show the BatchSaveToHDFS job in it's accepted and running status:

\begin{figure}[h!]
	\centering
	\includegraphics[height=0.4\textheight]{fig01/BatchSaveTOHDFS}
	\mycaption[BatchSaveToHDFS spark job in accepted status.]{BatchSaveToHDFS spark job in accepted status.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
The next figure show the job Hadoop Yarn UI status:
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/RunningJobStatus}
	\mycaption[Running job status Hadoop Yarn UI.]{Running job status Hadoop Yarn UI.}
	\label{fig:FilialesEtClients}
\end{figure}
\newline
The next figure show the Running application Master:
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/ApplicationMaster}
	\mycaption[Application Master dashboard.]{Application Master dashboard.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
The next figure show the running job log:
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.1\textheight]{fig01/JobLog}
	\mycaption[Job log.]{Job log.}
	\label{fig:FilialesEtClients}
\end{figure}

The next figure show the running job output:
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.4\textheight]{fig01/JobLogResualts}
	\mycaption[Job log Resualts.]{Job log Resualts.}
	\label{fig:FilialesEtClients}
\end{figure}
\newpage
\subsection{Show Resualts on Apache Zeppelin}
\label{sec:sec01}
The next figure show Analytics results displayed on Zeppelin dashboard :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/zeppelin}
	\mycaption[Zeppelin dashboard.]{Zeppelin dashboard.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsection{Account Managment}
\label{sec:sec01}
\subsubsection{Accounts}
\label{sec:sec01}
The next figure show the list Accounts :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/UserManagement}
	\mycaption[Account list.]{Account list.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsubsection{Account add}
\label{sec:sec01}
The next figure show the form add account :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/UserManagementAdd}
	\mycaption[Add account.]{Add account.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsubsection{Account Update}
\label{sec:sec01}
The next figure show the form update account :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/UserManagementUpdate}
	\mycaption[Update account.]{Update account.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsection{Promotion Managment}
\label{sec:sec01}
\subsubsection{Promotions}
\label{sec:sec01}
The next figure show the list Promotions :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/PromotionManagement}
	\mycaption[Promotions list.]{Promotions list.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsubsection{Promotion add}
\label{sec:sec01}
The next figure show the form add Promotion :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/PromotionAdd}
	\mycaption[Add promotions.]{Add promotions.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsubsection{Promotion Update}
\label{sec:sec01}
The next figure show the form update Promotion :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/PromotionUpdate}
	\mycaption[Update promotions.]{Update promotions.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsection{Display Analytics}
\label{sec:sec01}
The next figures show the Data Analytics Doshboard , this dashboard is update each time new data comes :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/Dashboard}
	\mycaption[Display analytics dashboard.]{Display analytics dashboard.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsection{Heat Map users}
\label{sec:sec01}
The next figure show the Data Analytics Doshboard , this dashboard is update each time new data comes :
\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3\textheight]{fig01/HeatMap}
	\mycaption[Heat map users dashboard.]{Heat map users dashboard.}
	\label{fig:FilialesEtClients}
\end{figure}
\subsection{Summary}
\label{sec:sec01}
In this chapter, we presented our project implementation. 
First, we have prepared the work environment which takes too much time configuring, adapting versions and dealing
 with system engineering issues. 
 Second, we ve moved to link each container to the other one. 
 Then we started making some jobs like the log producer, the BatchSaveToHDFS, the StreamingProcessing and the 
 Batch Processing. 
 Third, we ve implemented the microservices architecture that allows account (users) or promotions management.
  Finally, we have our results (Analytics, Cruds and Heat Map) displayed on a single dashboard.
\newpage
\section{General conclusion}
\label{sec:sec01}
This project was designed to handle scaling in term of features and treatment. Our project is still incomplete.
There is a lot of features that we would like to involve. Some of them are:
 \begin{itemize}
	\item A gateway api, that handle authentication using Spring boot and zuul.
	\item A container manager like docker Swarm or Kubernities that takes care of high availability and scalability.
	\item So much functional features like recommendation engine using Spark mllib.
	\item A mobile application that takes the log producer place.
  \item ...
\end{itemize}
Finally, We have been focused on creating a well designed system that can handle all of the requirements
 presented in the second chapter. The technical and the functional branches are now implemented and move
  to gather in a straight line. 

%=========================================================